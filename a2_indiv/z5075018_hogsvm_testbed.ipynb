{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, time\n",
    "from skimage import io\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import calibration\n",
    "import pickle\n",
    "import copy\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15,15)\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FH = None # global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'Individual_Component'\n",
    "SEED = 42\n",
    "\n",
    "# [HOG parameters]\n",
    "# img (w,h): (64,80)\n",
    "# NFEATURES = nblocks * norientations/cell * ncells/block\n",
    "POWER_LAW_COMPRESSION = True\n",
    "\n",
    "# BLOCK_SIZE_LIST = [1, 2, 3, 4]\n",
    "# BLOCK_NORM_LIST = ['L2-Hys'] # available: ['L1', 'L1-sqrt', 'L2', 'L2-Hys']\n",
    "# CELL_PIXEL_LIST = [4, 5, 6, 7, 8, 9, 10, 14]\n",
    "# ORIENTATION_LIST = [6, 8, 9, 10, 12]\n",
    "# NPOS_TRAINING_IMGS = 3000\n",
    "# NNEG_TRAINING_IMGS = 3000\n",
    "\n",
    "## (Test platform test runs)\n",
    "\n",
    "# BLOCK_SIZE_LIST = [1]\n",
    "# BLOCK_NORM_LIST = ['L1']\n",
    "# CELL_PIXEL_LIST = [4]\n",
    "# ORIENTATION_LIST = [8, 6]\n",
    "# NPOS_TRAINING_IMGS = 10\n",
    "# NNEG_TRAINING_IMGS = 10\n",
    "\n",
    "BLOCK_SIZE_LIST = [3]\n",
    "BLOCK_NORM_LIST = ['L2-Hys'] # available: ['L1', 'L1-sqrt', 'L2', 'L2-Hys']\n",
    "CELL_PIXEL_LIST = [4]\n",
    "ORIENTATION_LIST = [7,11]\n",
    "NPOS_TRAINING_IMGS = 3000\n",
    "NNEG_TRAINING_IMGS = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results file closed.\n"
     ]
    }
   ],
   "source": [
    "if RESULTS_FH != None:\n",
    "    RESULTS_FH.close()\n",
    "print('Results file closed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def list_mul(l, num):\n",
    "    return tuple(map(lambda x: int(num * x), l))\n",
    "\n",
    "# Histogram of Oriented Gradients\n",
    "def run_hog(img, visualize=False):\n",
    "    global NORIENTATIONS, CELL_PIXELSHAPE, CELLS_PER_BLOCK, BLOCK_NORM, POWER_LAW_COMPRESSION\n",
    "    result = hog(img, orientations=NORIENTATIONS, \n",
    "                 pixels_per_cell=CELL_PIXELSHAPE, cells_per_block=CELLS_PER_BLOCK, \n",
    "                 block_norm=BLOCK_NORM, visualize=visualize, transform_sqrt=POWER_LAW_COMPRESSION,\n",
    "                 multichannel=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/manikg/training-svm-classifier-with-hog-features\n",
    "\n",
    "def log_result(s, on_console=True): # debug log...\n",
    "    global RESULTS_FH\n",
    "    if on_console:\n",
    "        print(s)\n",
    "    print(s, file=RESULTS_FH)\n",
    "\n",
    "# Code to load the dataset\n",
    "def get_dataset_fp(is_train):\n",
    "    if is_train:\n",
    "        subroot = 'train'\n",
    "    else:\n",
    "        subroot = 'test'\n",
    "    base_fp = os.path.join(DIR, subroot)\n",
    "    pos_dirs = []\n",
    "    neg_dirs = []\n",
    "    for dir_name in os.listdir(base_fp): # all files & dirs\n",
    "        subfp = os.path.join(base_fp, dir_name)\n",
    "        if not os.path.isdir(subfp):\n",
    "            continue    \n",
    "        for subdir_name in os.listdir(subfp):\n",
    "            subsubfp = os.path.join(subfp, subdir_name)\n",
    "            if not os.path.isdir(subsubfp):\n",
    "                continue\n",
    "            if 'positive' in dir_name:\n",
    "                pos_dirs.append(subsubfp)\n",
    "            elif 'negative' in dir_name:\n",
    "                neg_dirs.append(subsubfp)\n",
    "    return pos_dirs, neg_dirs\n",
    "\n",
    "def get_subset_images(fp, nimgs=None, preproc=False):\n",
    "    results = []\n",
    "    results_fp = []\n",
    "    count = 0\n",
    "    for img_fn in os.listdir(fp):\n",
    "        img_fp = os.path.join(fp, img_fn)\n",
    "        if os.path.isdir(img_fp) or img_fn[-4:].lower() != '.pnm':\n",
    "            continue\n",
    "        img = io.imread(img_fp)\n",
    "        if preproc: # proprocess as HOG\n",
    "            hog_fd = run_hog(img)\n",
    "            results.append(hog_fd)\n",
    "        else:\n",
    "            results.append(img)\n",
    "        results_fp.append(img_fp)\n",
    "        count += 1 # restrict num imgs loaded\n",
    "        if nimgs != None and count >= nimgs:\n",
    "            break\n",
    "    return results, results_fp\n",
    "\n",
    "NO_PERSON = 0\n",
    "IS_PERSON = 1\n",
    "\n",
    "#def load_images(is_train, shuffle, npos_imgs=None, nneg_imgs=None, save_fn=None):\n",
    "def load_images(is_train, npos_imgs=None, nneg_imgs=None):\n",
    "    pos_dirs, neg_dirs = get_dataset_fp(is_train=is_train)    \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    fp_train = []\n",
    "    # get preprocessed training/testing data\n",
    "    for fp in pos_dirs:\n",
    "        start_time = time.time()\n",
    "        cur_subset, cur_subset_fp = get_subset_images(fp, npos_imgs, False)\n",
    "        x_train += cur_subset\n",
    "        fp_train += cur_subset_fp\n",
    "        log_result('  * {:.3f}s runtime (images loaded): {}'.format(time.time() - start_time,fp))\n",
    "        if npos_imgs != None:\n",
    "            npos_imgs -= len(cur_subset)\n",
    "            if npos_imgs <= 0:\n",
    "                break\n",
    "    pos_length = len(x_train)\n",
    "    y_train += [IS_PERSON] * pos_length\n",
    "    \n",
    "    for fp in neg_dirs:\n",
    "        start_time = time.time()\n",
    "        cur_subset, cur_subset_fp = get_subset_images(fp, nneg_imgs, False)\n",
    "        x_train += cur_subset\n",
    "        fp_train += cur_subset_fp\n",
    "        log_result('  * {:.3f}s runtime (images loaded): {}'.format(time.time() - start_time,fp))\n",
    "        if nneg_imgs != None:\n",
    "            nneg_imgs -= len(cur_subset)\n",
    "            if nneg_imgs <= 0:\n",
    "                break\n",
    "    y_train += [NO_PERSON] * (len(x_train) - pos_length)\n",
    "    return x_train, y_train, fp_train\n",
    "\n",
    "def images_to_hog(x_train, y_train, fp_train, shuffle, save_fn=None):\n",
    "    # convert each img to hog (intermed step added to cache original unproc'd images)\n",
    "    start_time = time.time()\n",
    "    for i in range(len(x_train)):\n",
    "        x_train[i] = run_hog(x_train[i])\n",
    "    hog_shape = x_train[0].shape\n",
    "    log_result('  * {:.3f}s runtime (conversion to HOG)'.format(time.time() - start_time))\n",
    "    \n",
    "    # shuffle training data\n",
    "    #print('Reformatting data...')\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    fp_train_index = np.arange(len(fp_train)) # create unique IDs (from 0)\n",
    "    \n",
    "    if shuffle:\n",
    "        y_train = y_train.reshape(len(y_train),1)\n",
    "        fp_train_index = fp_train_index.reshape(len(fp_train_index),1)\n",
    "        \n",
    "        data_frame = np.hstack((x_train,y_train, fp_train_index))\n",
    "        #print('Reshuffling data...')\n",
    "        start_time = time.time()\n",
    "        np.random.seed(SEED)\n",
    "        np.random.shuffle(data_frame)\n",
    "        x_train = data_frame[:,:-2]\n",
    "        y_train = data_frame[:,-2].ravel()\n",
    "        fp_train_index = data_frame[:,-1].ravel()\n",
    "        log_result('  * {:.3f}s runtime (shuffling)'.format(time.time() - start_time))\n",
    "    \n",
    "    log_result('HOG descriptor size: ' + str(hog_shape))\n",
    "    result = (x_train, y_train, fp_train_index, fp_train)\n",
    "    # save loaded images\n",
    "    if save_fn != None:\n",
    "        with open(save_fn, 'wb') as fh:\n",
    "            pickle.dump(result, fh)\n",
    "    return result\n",
    "    \n",
    "# Code to generate the SVM model\n",
    "def gen_model(model_fn, x_train, y_train):\n",
    "    # generate SVM model\n",
    "    start_time = time.time()\n",
    "    #clf = svm.SVC(probability=True)\n",
    "    #clf = linear_model.SGDClassifier()\n",
    "    clf = calibration.CalibratedClassifierCV(base_estimator = svm.LinearSVC(loss='hinge'), cv=5)\n",
    "    clf.fit(x_train, y_train)\n",
    "    log_result('{:.3f}s runtime (SGD training)'.format(time.time() - start_time))\n",
    "    \n",
    "    # save SVM model\n",
    "    with open(model_fn, 'wb') as fh:\n",
    "        pickle.dump(clf, fh)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes statistics for the classifier's performance\n",
    "def run_test(clf, x_test, y_test):\n",
    "    start_time = time.time()\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_prob = clf.predict_proba(x_test)\n",
    "        \n",
    "    # y_prob: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    #  - shape: (nsamples, nclasses), by order of `clf.classes_`\n",
    "    elapsed_time = time.time() - start_time\n",
    "    auc = metrics.roc_auc_score(y_test, y_prob[:,IS_PERSON], average='weighted')\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "    stats = {'Prediction Time':elapsed_time, 'AUC':auc, 'Accuracy':accuracy, 'Precision':precision, \n",
    "             'Recall':recall, 'Confusion Matrix':confusion_matrix}\n",
    "    \n",
    "    log_result('  * {:.6f}s prediction time ({:.6f} s/image)'.format(elapsed_time, elapsed_time/len(y_test)))\n",
    "    log_result('  * AUC (weighted): {:.9f}'.format(auc))\n",
    "    log_result('  * Accuracy: {:.9f}'.format(accuracy))\n",
    "    log_result('  * Precision (weighted): {:.9f}'.format(precision))\n",
    "    log_result('  * Recall (weighted): {:.9f}'.format(recall))\n",
    "    log_result('  * Confusion Matrix:')\n",
    "    log_result(str(confusion_matrix))\n",
    "    return y_pred, y_prob, stats\n",
    "\n",
    "# Returns all image file paths detected falsely\n",
    "def filter_failed_fp(y_pred, y_test, fp_test_index, fp_test):\n",
    "    failed_fp = []\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] != y_test[i]:\n",
    "            if y_pred[i] == NO_PERSON:\n",
    "                label = 'FN'\n",
    "            else:\n",
    "                label = 'FP'\n",
    "            failed_fp.append( (label, fp_test[int(fp_test_index[i])]) )\n",
    "    return failed_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate SVM model (and training images) or load cached result\n",
    "def get_svm_model(cur_dir_contents, SVM_MODEL, PRE_TRAINING_IMGS):\n",
    "    global NPOS_TRAINING_IMGS,NNEG_TRAINING_IMGS\n",
    "    global CACHED_UNPROC_TRAINING # training\n",
    "    if SVM_MODEL not in cur_dir_contents:\n",
    "        log_result('\\n[Training SVM model]:')\n",
    "        # get unprocessed images\n",
    "        if CACHED_UNPROC_TRAINING == None: \n",
    "            x_train, y_train, fp_train = load_images(True, NPOS_TRAINING_IMGS,NNEG_TRAINING_IMGS)\n",
    "            CACHED_UNPROC_TRAINING = (copy.deepcopy(x_train), copy.deepcopy(y_train), copy.deepcopy(fp_train))\n",
    "        else:\n",
    "            log_result('  * Loading cached unprocessed images from RAM')\n",
    "            start_time = time.time()\n",
    "            x_train, y_train, fp_train = copy.deepcopy(CACHED_UNPROC_TRAINING)\n",
    "            log_result('    {:.3f}s loading time'.format(time.time() - start_time))\n",
    "        # process images, generate model\n",
    "        x_train, y_train, fp_train_index, fp_train = images_to_hog(x_train, y_train, fp_train, True, save_fn=PRE_TRAINING_IMGS)\n",
    "        clf = gen_model(SVM_MODEL,x_train,y_train)\n",
    "    else:\n",
    "        log_result('\\n[Loading cached SVM model & training images]:')\n",
    "        with open(SVM_MODEL, 'rb') as fh:\n",
    "            clf = pickle.load(fh)\n",
    "        with open(PRE_TRAINING_IMGS, 'rb') as fh:\n",
    "            x_train, y_train, fp_train_index, fp_train = pickle.load(fh)\n",
    "            \n",
    "    log_result('Number of training images loaded: {}'.format(len(x_train)))\n",
    "    return x_train, y_train, fp_train_index, fp_train, clf\n",
    "\n",
    "# 2. Generate test images or load cached result\n",
    "def get_test_imgs(cur_dir_contents, PRE_TESTING_IMGS):\n",
    "    global CACHED_UNPROC_TESTING  # testing\n",
    "    # Generate test images or load cached result\n",
    "    if PRE_TESTING_IMGS not in cur_dir_contents:\n",
    "        log_result('\\n[Generating test images]:')\n",
    "        # get unprocessed images\n",
    "        if CACHED_UNPROC_TESTING == None:\n",
    "            x_test, y_test, fp_test = load_images(False)\n",
    "            CACHED_UNPROC_TESTING = (copy.deepcopy(x_test), copy.deepcopy(y_test), copy.deepcopy(fp_test))\n",
    "        else:\n",
    "            log_result('  * Loading cached unprocessed test images from RAM')\n",
    "            start_time = time.time()\n",
    "            x_test, y_test, fp_test = copy.deepcopy(CACHED_UNPROC_TESTING)\n",
    "            log_result('    {:.3f}s loading time'.format(time.time() - start_time))\n",
    "        # process images, generate model\n",
    "        x_test, y_test, fp_test_index, fp_test = images_to_hog(x_test, y_test, fp_test, True, save_fn=PRE_TESTING_IMGS)\n",
    "    else:\n",
    "        log_result('\\n[Loading cached test images]:')\n",
    "        with open(PRE_TESTING_IMGS, 'rb') as fh:\n",
    "            x_test, y_test, fp_test_index, fp_test = pickle.load(fh)\n",
    "    \n",
    "    log_result('Number of testing images loaded: {}'.format(len(x_test)))\n",
    "    return x_test, y_test, fp_test_index, fp_test\n",
    "            \n",
    "# 3. Evaluate performance (for all permutations)\n",
    "#    call: run_test()\n",
    "#    manual post-analysis: filter_failed_fp()\n",
    "\n",
    "# Wrapper for single iteration. Note: all CSV write logic to be contained here. (hack)\n",
    "def run_hogsvm():\n",
    "    global NORIENTATIONS, CELL_PIXELSHAPE, CELLS_PER_BLOCK, BLOCK_NORM, POWER_LAW_COMPRESSION\n",
    "    global RESULTS_FH, CSV_LINE\n",
    "    base_fn = '_ori({})_cellpix({})_blksze({})_blknrm({})'.format(\n",
    "        NORIENTATIONS, CELL_PIXELSHAPE[0], CELLS_PER_BLOCK[0], BLOCK_NORM)\n",
    "    pickle_type = '.pickle'\n",
    "    text_type = '.txt'\n",
    "    SVM_MODEL = 'hogsvm_model' + base_fn + pickle_type\n",
    "    PRE_TRAINING_IMGS = 'hogsvm_train' + base_fn + pickle_type\n",
    "    PRE_TESTING_IMGS = 'hogsvm_test' + base_fn + pickle_type\n",
    "    results_fn = 'hogsvm_result' + base_fn + text_type\n",
    "    \n",
    "    cur_dir_contents = os.listdir('.')\n",
    "    RESULTS_FH = open(results_fn, 'w')\n",
    "    log_result('[Current parameter sweep]:')\n",
    "    log_result('  * Number of orientations: {}'.format(NORIENTATIONS))\n",
    "    log_result('  * Cell pixel shape: {}'.format(CELL_PIXELSHAPE))\n",
    "    log_result('  * Number of cells per block: {}'.format(CELLS_PER_BLOCK))\n",
    "    log_result('  * Block normalisation method: {}'.format(BLOCK_NORM))\n",
    "    log_result('  * Power law compression (preprocessing) on: {}'.format(POWER_LAW_COMPRESSION))\n",
    "    # (NOTE: actually, power law is square root --> slightly different method)\n",
    "    \n",
    "    # load svm model and test dataset\n",
    "    x_train, y_train, fp_train_index, fp_train, clf = get_svm_model(cur_dir_contents, SVM_MODEL, PRE_TRAINING_IMGS)\n",
    "    x_test, y_test, fp_test_index, fp_test          = get_test_imgs(cur_dir_contents, PRE_TESTING_IMGS)\n",
    "    # * (CSV): get hog shape (duplicated logic)\n",
    "    CSV_LINE.append(x_train.shape[1]) # 'Feature Size'\n",
    "    \n",
    "    # evaluate on test and training datasets (as a crude check for overfitting)\n",
    "    log_result('\\n[Classifier statistics (on test data)]:')\n",
    "    y_pred, y_prob, stats_test = run_test(clf, x_test, y_test)\n",
    "    log_result('\\n[Classifier statistics (on training data)]:')\n",
    "    y_pred_training, y_prob_training, stats_train = run_test(clf, x_train, y_train)\n",
    "    # * (CSV): add stats\n",
    "    CSV_LINE += [stats_test['AUC'], stats_test['Accuracy'], stats_test['Precision'], \n",
    "                 stats_test['Recall'], stats_train['Accuracy'], stats_test['Prediction Time']]\n",
    "    tn_teststat, fp_teststat, fn_teststat, tp_teststat = stats_test['Confusion Matrix'].ravel()\n",
    "    CSV_LINE += [tp_teststat, tn_teststat, fp_teststat, fn_teststat]\n",
    "    \n",
    "    # identify all false results\n",
    "    failed_fp = filter_failed_fp(y_pred, y_test, fp_test_index, fp_test)\n",
    "    log_result('\\n[Falsely detected images]:')\n",
    "    for label, fp in failed_fp:\n",
    "        log_result('  * {}: {}'.format(label,fp), on_console=False)\n",
    "\n",
    "    RESULTS_FH.close()\n",
    "    RESULTS_FH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####[TEST 1/2]##################################################################\n",
      "[Current parameter sweep]:\n",
      "  * Number of orientations: 7\n",
      "  * Cell pixel shape: (4, 4)\n",
      "  * Number of cells per block: (3, 3)\n",
      "  * Block normalisation method: L2-Hys\n",
      "  * Power law compression (preprocessing) on: True\n",
      "\n",
      "[Training SVM model]:\n",
      "  * 1.822s runtime (images loaded): Individual_Component\\train\\train_positive_A\\00000000\n",
      "  * 1.700s runtime (images loaded): Individual_Component\\train\\train_positive_A\\00000001\n",
      "  * 1.619s runtime (images loaded): Individual_Component\\train\\train_positive_A\\00000002\n",
      "  * 1.715s runtime (images loaded): Individual_Component\\train\\train_negative_A\\00000000\n",
      "  * 1.793s runtime (images loaded): Individual_Component\\train\\train_negative_A\\00000001\n",
      "  * 1.776s runtime (images loaded): Individual_Component\\train\\train_negative_A\\00000002\n",
      "  * 129.436s runtime (conversion to HOG)\n",
      "  * 0.423s runtime (shuffling)\n",
      "HOG descriptor size: (15876,)\n",
      "50.449s runtime (SGD training)\n",
      "Number of training images loaded: 6000\n",
      "\n",
      "[Generating test images]:\n",
      "  * 1.791s runtime (images loaded): Individual_Component\\test\\test_positive\\00000000\n",
      "  * 1.740s runtime (images loaded): Individual_Component\\test\\test_positive\\00000001\n",
      "  * 1.815s runtime (images loaded): Individual_Component\\test\\test_positive\\00000002\n",
      "  * 0.855s runtime (images loaded): Individual_Component\\test\\test_positive\\00000003\n",
      "  * 2.290s runtime (images loaded): Individual_Component\\test\\test_negative\\00000000\n",
      "  * 1.791s runtime (images loaded): Individual_Component\\test\\test_negative\\00000001\n",
      "  * 1.898s runtime (images loaded): Individual_Component\\test\\test_negative\\00000002\n",
      "  * 1.719s runtime (images loaded): Individual_Component\\test\\test_negative\\00000003\n",
      "  * 1.787s runtime (images loaded): Individual_Component\\test\\test_negative\\00000004\n",
      "  * 1.688s runtime (images loaded): Individual_Component\\test\\test_negative\\00000005\n",
      "  * 218.994s runtime (conversion to HOG)\n",
      "  * 0.865s runtime (shuffling)\n",
      "HOG descriptor size: (15876,)\n",
      "Number of testing images loaded: 9457\n",
      "\n",
      "[Classifier statistics (on test data)]:\n",
      "  * 20.801462s prediction time (0.002200 s/image)\n",
      "  * AUC (weighted): 0.999181564\n",
      "  * Accuracy: 0.988579888\n",
      "  * Precision (weighted): 0.988610169\n",
      "  * Recall (weighted): 0.988579888\n",
      "  * Confusion Matrix:\n",
      "[[5934   66]\n",
      " [  42 3415]]\n",
      "\n",
      "[Classifier statistics (on training data)]:\n",
      "  * 13.137770s prediction time (0.002190 s/image)\n",
      "  * AUC (weighted): 1.000000000\n",
      "  * Accuracy: 1.000000000\n",
      "  * Precision (weighted): 1.000000000\n",
      "  * Recall (weighted): 1.000000000\n",
      "  * Confusion Matrix:\n",
      "[[3000    0]\n",
      " [   0 3000]]\n",
      "\n",
      "[Falsely detected images]:\n",
      "\n",
      "####[TEST 2/2]##################################################################\n",
      "[Current parameter sweep]:\n",
      "  * Number of orientations: 11\n",
      "  * Cell pixel shape: (4, 4)\n",
      "  * Number of cells per block: (3, 3)\n",
      "  * Block normalisation method: L2-Hys\n",
      "  * Power law compression (preprocessing) on: True\n",
      "\n",
      "[Training SVM model]:\n",
      "  * Loading cached unprocessed images from RAM\n",
      "    0.154s loading time\n",
      "  * 135.137s runtime (conversion to HOG)\n",
      "  * 0.786s runtime (shuffling)\n",
      "HOG descriptor size: (24948,)\n",
      "63.021s runtime (SGD training)\n",
      "Number of training images loaded: 6000\n",
      "\n",
      "[Generating test images]:\n",
      "  * Loading cached unprocessed test images from RAM\n",
      "    0.315s loading time\n",
      "  * 210.386s runtime (conversion to HOG)\n",
      "  * 1.163s runtime (shuffling)\n",
      "HOG descriptor size: (24948,)\n",
      "Number of testing images loaded: 9457\n",
      "\n",
      "[Classifier statistics (on test data)]:\n",
      "  * 31.743732s prediction time (0.003357 s/image)\n",
      "  * AUC (weighted): 0.999305756\n",
      "  * Accuracy: 0.990166015\n",
      "  * Precision (weighted): 0.990189037\n",
      "  * Recall (weighted): 0.990166015\n",
      "  * Confusion Matrix:\n",
      "[[5943   57]\n",
      " [  36 3421]]\n",
      "\n",
      "[Classifier statistics (on training data)]:\n",
      "  * 20.518120s prediction time (0.003420 s/image)\n",
      "  * AUC (weighted): 1.000000000\n",
      "  * Accuracy: 1.000000000\n",
      "  * Precision (weighted): 1.000000000\n",
      "  * Recall (weighted): 1.000000000\n",
      "  * Confusion Matrix:\n",
      "[[3000    0]\n",
      " [   0 3000]]\n",
      "\n",
      "[Falsely detected images]:\n"
     ]
    }
   ],
   "source": [
    "# [PARAMETER SWEEP]\n",
    "ntests = len(BLOCK_SIZE_LIST) * len(BLOCK_NORM_LIST) * len(CELL_PIXEL_LIST) * len(ORIENTATION_LIST)\n",
    "count = 1\n",
    "test_start_time = time.time()\n",
    "\n",
    "CACHED_UNPROC_TRAINING = None\n",
    "CACHED_UNPROC_TESTING = None\n",
    "CSV_LINE = None\n",
    "\n",
    "csv_fh = open('hogsvm_result_all.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(csv_fh)\n",
    "\n",
    "csv_headers = ['Test', 'Cells per Block', 'Pixels per Cell', 'Orientations', \n",
    "               'Feature Size', 'AUC', 'Accuracy', 'Precision', 'Recall', \n",
    "               'Training Accuracy', 'Prediction Time (s)',\n",
    "               'TP', 'TN', 'FP', 'FN'] # must write to in same order\n",
    "csv_writer.writerow(csv_headers)\n",
    "\n",
    "# block parameters\n",
    "for block_size in BLOCK_SIZE_LIST:\n",
    "    CELLS_PER_BLOCK = (block_size, block_size)\n",
    "    for BLOCK_NORM in BLOCK_NORM_LIST:\n",
    "        # cell/orientation parameters\n",
    "        for cp in CELL_PIXEL_LIST:\n",
    "            CELL_PIXELSHAPE = (cp,cp)\n",
    "            for NORIENTATIONS in ORIENTATION_LIST:\n",
    "                print('\\n####[TEST {}/{}]##################################################################'.format(count,ntests))\n",
    "                CSV_LINE = [count, block_size, cp, NORIENTATIONS]\n",
    "                run_hogsvm()\n",
    "                csv_writer.writerow(CSV_LINE)\n",
    "                count += 1\n",
    "# close file\n",
    "csv_fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046.850s elapsed (entire test)\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "if RESULTS_FH != None:\n",
    "    RESULTS_FH.close()\n",
    "print('{:.3f}s elapsed (entire test)'.format(time.time() - test_start_time))\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
